---
layout: default
latex: true
---

<h1>Table of Contents</h1>

<ul>
 <li><a href="#Introduction">Introduction</a></li>
 <li><a href="#SSAO">Screen Space Ambient Occlusion</a></li>
 <li><a href="#Algorithm">Algorithm</a></li>
 <li><a href="#RealtimeRendering">Real-time rendering technique</a></li>
 <li><a href="#Conclusion">Conclusion and future directions</a></li>
 <li><a href="#Bibliography">Bibliography</a></li>
</ul>

<a name="Introduction" style="color: #f0e7d5"><h1>Introduction</h1></a>

<p>
Ambient occlusion for a point in space is a measure of how much such point is hidden by
its surrounding environment: the higher the occlusion value, the less amount of
ambient light will hit the point. Attempts have been made to give a mathematical
description of ambient occlusion, the first mathematical formalization
of something similar to ambient occlusion I know about is contained in
<a href="#ZhukovEtAl1998"><em>Zhukov et al.</em></a>.
</p>
<p>
Ambient occlusion is a cheap way of approximating global illumination effects used
to give a photo-realistic appearance to synthetically generated images. Note that global
illumination techniques are not just useful to produce nice looking images
but are also a way to make a viewer understand the actual spatial shape of objects 
<a href="#aoandoao">(Fig. 1)</a>.
</p>

<p style="text-align: center">
<a name="aoandnoao"><img style="text-align: center; width: 650px" src="AO_AND_NOAO.png"/></a>
<small>Fig. 1: real-time raytracing of chemical data with(right) and without(left) ambient occlusion.</small>
</p>

<p>
Techniques smilar to the ones used to compute ambient occlusion 
can be adapted to approximate radiance transfer and generate color-bleeding effects in screen space.
</p>
<p>
Different methods for computing ambient occlusion have been presented over the
past years; most of them fall into one of two categories: 
<ol>
 <li>per-vertex or per-face ambient occlusion pre-computation</li>
 <li>real-time per-pixel computation</li>
</ol>
</p>
<p>
Vertex or face based pre-computation methods are fine for static geometry with a high polygon
count while per-pixel techniques work well for dynamic scenes, do not require the input
to be a set of polygons, and therefore integrate well with ray-tracing and volume rendering. 
</p>
<p>
The method described in this article is a per-pixel technique which was implemented
between 2007 and 2008 as an add-on to a real-time ray-tracer for large molecular models
and further modified to work with any type of data.
The key technique employed by the algorithm is to use the angular coefficient of rays
to select the points that contribute to the overall occlusion.
</p>

<a name="SSAO" style="color: #f0e7d5">
<h1>Screen Space Ambient Occlusion</h1></a>

<p>
Screen space ambient occlusion for a pixel $p$ can be approximated by 
computing the average of occlusion contributions coming from a number of directions:
<a name="ssao" style="color: #f0e7d5">
$$
A_{p} = \frac{1}{N}\sum_{i=1}^N A_{pi}
$$
</a>

</p>
<p>
Where $A_{p}$ is the total occlusion at $p(x,y)$ and $A_{pi}$ is the contribution 
coming from direction $i$ to the total occlusion.
</p>

<p style="text-align: center">
<a name="ssaoscreenspace"><img src="SSAO-screenspace-w.png" style="width: 650px"/></a>
<small>Fig. 2 Screen space depth profile.</small>
</p>

<p>
The term $A_{pi}$ is computed by integrating the ambient occlusion along a linear path
in screen space:
<a name="ssaoray" style="color: #f0e7d5">
$$
A_{pi} = \frac{1}{S}\sum_{j=1}^Socc(\vec{p},\vec{o}_{ij})
$$
</a>

Where:

<ul>
 <li>$S$ is the total number of steps </li>
 <li>$\vec{p}$ and $\vec{o}$ are $3D$\footnote{ 2D pixel coordinates + 1D depth coordinate} screen coordinates</li>
 <li>$occ(\vec{a}, \vec{b})$ is the contribution from point $\vec{b}$ to the occlusion
	   of point $\vec{a}$ </li>
</ul>
</p>

<p>
The contribution to the total occlusion of a point in screen space from another point in 
screen space is approximated by first transforming the two points into world space then applying the 
formula:
<p style="text-align: center">
<a name="occ" style="color: #f0e7d5">
$$
occ(\vec{p},\vec{o}) = \frac{max\left( 0, \hat{N}_{P} \cdot \frac{\vec{O}-\vec{P}}{\left|\vec{O} - \vec{P}\right|}\right)}
{f\left( \left|\vec{O} - \vec{P}\right|\right)}
$$
</a>
<small>Equation (1)</small>
</p>
                                 
Where:
<ul>
 <li>$\vec{O}$ and $\vec{P}$ are the world space coordinates of $\vec{o}$ and $\vec{p}$</li>
 <li>$\hat{N}$ is the unit normal in world coordinates at point $\vec{P}$</li>
 <li>$f(d)$ is a (usually polynomial) function of the distance $d$ between the two points</li> 
</ul>
</p>

<p>
Note that several variations are possible here: the normal at $\vec{O}$ can be used instead of 
$\frac{\vec{O}-\vec{P}}{\left|\vec{O} - \vec{P}\right|}$;
all the computation can be carried on in screen space; the normals can be precomputed or computed on the fly
from the tangent space.
</p>
<p>
Also it is possible to use a varying-step approach where the step increases with the distance (e.g. $step(d)=kd^2$) to
take into account high spatial frequencies only at close distance; this allows to take into account contributions from
both near and far fragments while still running at reasonable frame rates.
</p>
<p>
$f(d)$ can be e.g.:
$$
f(d) = B(d_{max})d^2 + 1
$$
Where $B(d_{max})$ is a coefficient dependent on the maximum ray length. The value assigned to $B$ ensures that
$occ(\vec{p}, \vec{o}_{d_{max}})$ is always equal to a predefined occlusion value.  
</p>

<a name="Algorithm" style="color: #f0e7d5">
<h1>Algorithm</h1></a>

<p>
According to the equations defined in the previous sections ambient occlusion at each pixel can be
computed by selecting a number of directions in $2D$ screen space and averaging the occlusion contributions 
along each direction.
</p>

<p style="text-align: center">
<a name="ssaorays" ><img src="SSAO-rays.png" style="width: 325px"/></a><br/>
<small>Fig. 3 Rays in screen space.</small>
</p>

<p>
The occlusion along a specific direction is computed by averaging the contributions of each point found along the 
$2D$ ray shot in that direction; now, in order to minimize the number of times <a href="#occ">Eq. 1</a>
is applied we need to understand if a point along the ray is at all visible from the ray starting point (the pixel being shaded) 
i.e. if a line connecting the two points does not intersect any geometry. It turns out this is quite easy 
to understand by comparing the angular coefficient of the $3D$ line connecting the ray starting point with the 
previous intersection point and the one of the $3D$ line connecting the current point with the ray staring point.
</p>

<p>
The algorithm to compute the per-pixel occlusion is therefore:
<ol>
 <li> <em>FOR EACH</em> pixel $\vec{p}$ select directions in $2D$ screen coordinates</li>
 <li> <em>FOR EACH</em> direction follow a linear path up to <em>MAX_DISTANCE</em> in that direction and</li>
 <li> <em>FOR EACH</em> $2D$ point $\vec{o}$ along the path compute the angular coefficient
      $c = \frac{\left(z_{p} - z_{o}\right)}{\left|\vec{o} - \vec{p}\right|}$ where
      $z_{o}$ and $z_{p}$ are the depths of pixels $\vec{o}$ and $\vec{p}$</li> 
<li> <em>IF</em> the newly computed coefficient is greater than the previously computed one</li>
<li> <em>THEN</em> replace the previously computed coefficient with the new one and add the contribution
      of the current point to the total occlusion by applying <a href="#occ">Eq. 1</a>.
</ol>
<p style="text-align: center">
<a name="ssaoangularcoeff"><img src="SSAO-angularcoeff-w.png" style="width: 650px"/></a>
<small>Fig. 4 Slope(angular coefficient) of rays.</small>
</p>
</p>
<p>
The <em>MAX_DISTANCE</em> length of the ray in pixels is computed as a percentage of the object/scene bounding sphere 
radius projected into screen space.
In cases where the scene completely fills the view it is actually easier to use a percentage of the view size. 
</p>
<p>
Note that the value $\left(z_{p} - z_{o}\right)$ is negative for points $\vec{o}$ that are farther away 
from then viewer than point $\vec{p}$.
</p>

<a name="RealtimeRendering" style="color: #f0e7d5">
<h1>Real-time rendering technique</h1></a> 

<p>
The data needed by the algorithm described in the previous section are:

<ul>
 <li>depth information for the entire scene</li>
 <li>item world space coordinates of each fragment (pixel)</li>
 <li>normalized world space normals of each fragment (pixel)</li> 
</ul>

which can all be computed in a single pre-rendering step by using OpenGL's multiple-render-target feature.
</p>
<p>
Real-time ambient occlusion computation is therefore performed in two separate rendering stages:

<ol>
 <li>store pixel world positions, normals and depths in texture targets</li>
 <li>compute per-pixel ambient occlusion (and shade) from stored data</li> 
</ol>

<p style="text-align: center">
<a name="ssaodiagram"><img src="SSAO-diagram-w.png" style="width: 650px"/></a>
<small>Fig. 5 Architecture diagram of rendering engine.</small>
</p>

</p>

<a name="Conclusion" style="color: #f0e7d5">
<h1>Conclusion and future directions</h1></a>

<p>
Screen space ambient occlusion can be successfully used to approximate global illumination effects, depending 
on the intended usage it might however show serious limitations, the biggest of which is the inability to take
into account objects which lay outside the current view frustum; techniques can be applied to
mitigate this problem such as using a different (bigger) frustum for the pre-rendering stage or implementing methods similar to reflection mapping.
Note however that the ability to use only the visible geometry for performing ambient occlusion computation can indeed
be desirable in cases where the viewpoint is actually inside the model to explore as it is the case for architectural
walkthroughs, interior designs or scientific visualization in general. 
</p>
<p>
The plan is to use the described technique to perform radiance-transfer like behavior in a new volume/ray-casting
based rendering engine currently in development. The basic idea is to implement a  multi-stage algorithm
where the first stage takes care of illuminating objects visible from light sources and the subsequent stages to compute
ambient light from previously illuminated geometry.
</p>
<p>
Also I plan to extend a previously developed ambient occlusion generation application with a variation of this
algorithm to generate pre-computed cube and spherical ambient occlusion maps.  
</p>

<a name="Bibliography" style="color: #f0e7d5">
<h1>Bibliography</h1></a>

<p>
<a name="BavoilSainz2009">Bavoil L. and Sainz M.</a><br/>
<em>Multi-Layer Dual-Resolution Screen-Space Ambient Occlusion</em>,<br/>
ACM SIGGRAPH 2009 Talk Program.
</p>
<p>
<a name="Ritschel">T. Ritschel, T. Grosch and H. P. Seidel</a>,<br/>
<em>Approximating Dynamic Global Illumination in Image Space</em>,<br/>
MPI Informatik
</p>                                                                                             
<a name="Sainz">Sainz, M.</a>,</br>
<em>Real-Time Depth Buffer Based Ambient Occlusion</em><br/> 
Game Developer Conference 2008, San Francisco.
</p>
<p>
<a name="ShanmugamArikan2007">Perumaal Shanmugam and Okan Arikan</a><br/>
<em>Hardware accelerated ambient occlusion techniques on GPUs</em>,<br/>
Proceedings of the 2007 symposium on Interactive 3D graphics and games, Seattle.  
</p>
<p>
<a name="TatarchukGDC2006">Tatarchuk N.</a></br>
<em>Parallax Occlusion Mapping for Highly Detailed Surface Rendering</em></br>
Game Developer Conference 2006, San Francisco.
</p>
<a name="Kontkanen2005i3d">Kontkanen J. and Laine S.</a><br/> 
<em>Ambient Occlusion Fields</em>,</br> 
Proceedings of ACM SIGGRAPH 2005 Symposium on Interactive 3D Graphics and Games,<br/>
Pages 41--48, 2005 ACM Press.
</p>
<p>
<a name="ZhukovEtAl1998">Zhukov S., Iones A., Kronin G.</a><br/> 
<em>"An ambient light illumination model</em>,<br/>
Rendering techniques '98:<br/>
proceedings of the Eurographics Workshop, Vienna - AT, 1998.
</p>



